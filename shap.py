# -*- coding: utf-8 -*-
"""MLOpsAssignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CThGz9oSV29ssCGkQBMShEV-bzStlx6b
"""

!pip install dataprep

import numpy as np
import pandas as pd
from dataprep.datasets import load_dataset
from dataprep.eda import create_report
from dataprep.eda.missing import plot_missing
from sklearn.preprocessing import MinMaxScaler

# Step 1: Load the Dataset
df = load_dataset("titanic")

# Step 2: Data Cleaning
# Replace placeholder for missing values with NaN
df = df.replace(" ?", np.NaN)

# Step 3: Exploratory Data Analysis (EDA)
# Generate a report to understand the dataset
report = create_report(df)
report.show_browser()

# Step 4: Drop Irrelevant Columns
# Drop columns that are not needed for modeling
df = df.drop(columns=['PassengerId', 'Name', 'Ticket'])

# Step 5: Visualize Missing Values
# Plot missing values to understand which features need handling
plot_missing(df)

# Step 6: Drop Columns with High Percentage of Missing Values
# Drop the 'Cabin' column due to too many missing values
df = df.drop(columns=['Cabin'])

# Step 7: Handle Missing Values
# Fill missing values in 'Age' with the median
df['Age'] = df['Age'].fillna(df['Age'].median())
# Fill missing values in 'Embarked' with the most frequent value (mode)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# Step 8: Outlier Detection and Removal
# Handle outliers in 'Fare' using the IQR method
Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['Fare'] = np.where(df['Fare'] < lower_bound, lower_bound, df['Fare'])
df['Fare'] = np.where(df['Fare'] > upper_bound, upper_bound, df['Fare'])

# Step 9: Feature Engineering
# Create new feature 'FamilySize'
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
# Drop 'SibSp' and 'Parch' after creating 'FamilySize'
df = df.drop(columns=['SibSp', 'Parch'])

# Create new feature 'AgeGroup'
bins = [0, 18, 35, 60, np.inf]
labels = ['child', 'young_adult', 'adult', 'elderly']
df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)
# Drop the original 'Age' column
df = df.drop(columns=['Age'])

# Step 10: Encode Categorical Variables
# One-hot encoding for 'Sex', 'Embarked', and 'AgeGroup'
encoded_cols = ['Sex', 'Embarked', 'AgeGroup']
df = pd.get_dummies(df, columns=encoded_cols, drop_first=True)  # drop_first=True to avoid multicollinearity

# Step 11: Scale Numerical Features
# Scale 'Fare' and 'FamilySize' using Min-Max Scaler
scaler = MinMaxScaler()
df[['Fare', 'FamilySize']] = scaler.fit_transform(df[['Fare', 'FamilySize']])

# NEW Step: Save the preprocessed DataFrame to a CSV file
df.to_csv('titanic_preprocessed.csv', index=False)
print("Preprocessed data saved to 'titanic_preprocessed.csv'")

!pip install tpot

# model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from tpot import TPOTClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pickle  # Import pickle for saving the model

# Load the preprocessed Titanic dataset
df = pd.read_csv('/content/titanic_preprocessed.csv')

# Step 1: Define Features and Target
# Assuming 'Survived' is the target column
X = df.drop(columns=['Survived'])  # Features
y = df['Survived']  # Target (Survival label)

# Step 2: Split the Data into Training and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Initialize TPOT Classifier (AutoML)
tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)

# Step 4: Train the AutoML Model
tpot.fit(X_train, y_train)

# Step 5: Evaluate the Model on the Test Set
accuracy = tpot.score(X_test, y_test)
print(f"TPOT AutoML Model Accuracy: {accuracy}")

# Step 6: Generate Predictions and Evaluation Metrics
y_pred = tpot.predict(X_test)
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Step 7: Export the Best Model as Python Code
tpot.export('best_titanic_pipeline.py')
print("Best model exported to 'best_titanic_pipeline.py'")

# NEW Step: Save the TPOT model as a pickle file
with open('best_titanic_model.pkl', 'wb') as f:
    pickle.dump(tpot.fitted_pipeline_, f)  # Save the fitted pipeline
print("Best model saved to 'best_titanic_model.pkl'")

!pip install shap

import shap

# Load the saved model from the pickle file
with open('best_titanic_model.pkl', 'rb') as file:
    model = pickle.load(file)

# Access the fitted model within the pipeline
model = model.steps[-1][1]

# Use SHAP with the loaded model
explainer = shap.Explainer(model, X_train)

# Generate SHAP values for the test set
shap_values = explainer(X_test)

# SHAP summary plot to visualize feature importance
shap.summary_plot(shap_values, X_test)

# SHAP waterfall plot for the first prediction
shap.plots.waterfall(shap_values[0])